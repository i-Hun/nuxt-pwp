{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ансамбль методов в статистике и обучении машин использует несколько обучающих алгоритмов с целью получения лучшей эффективности прогнозирования, чем могли бы получить от каждого обучающего алгоритма по отдельности.\n",
    "\n",
    "* Ансамбли более гибкие, они обладают большим числом параметров, чем отдельные модели.\n",
    "    * В теории, из-за этого они склонные переобучаться, но на практике наоборот.\n",
    "* Вычисление предсказания ансамбля обычно требует больше вычислений, чем предсказание одной модели.\n",
    "* Ансамбли склонны давать результаты лучше, если имеется существенное отличие моделей. Если модели очень похожи, то качество итоговой модели может быть даже хуже, т.к. ошибки усиливаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Простое усреднение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble=VotingClassifier(estimators=[('Decision Tree', decisiontree), ('Random Forest', forest)], \n",
    "                       voting='soft', weights=[2,1]).fit(train_X,train_Y)\n",
    "print('The accuracy for DecisionTree and Random Forest is:',ensemble.score(test_X,test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Блендинг или усреднение (можно взвешенное)\n",
    "\n",
    "Устредняем результаты нескольких моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Бэггинг\n",
    "\n",
    "Бэггинг – технология классификации, где в отличие от бустинга все элементарные классификаторы обучаются и работают параллельно (независимо друг от друга). Идея заключается в том, что классификаторы не исправляют ошибки друг друга, а компенсируют их при голосовании. Базовые классификаторы должны быть независимыми, это могут быть классификаторы основанные на разных группах методов или же обученные на независимых наборах данных. Во втором случае можно использовать один и тот же метод. Бэггинг позволяет снизить процент ошибки классификации в случае, когда высока дисперсия ошибки базового метода.\n",
    "\n",
    "Метод бэггинга (bagging, bootstrap aggregation) был предложен [Л. Брейманом в 1996 году](https://www.stat.berkeley.edu/~breiman/bagging.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/twocart.png)\n",
    "Пример с сайта XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Преимущества\n",
    "* Помогает против переобучения.\n",
    "* Улучшает качество.\n",
    "* Хорошо распараллеливается.\n",
    "\n",
    "Недостатки\n",
    "* Требует больше времени.\n",
    "* Не используется весь набор данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Параметры бэггинга\n",
    "* Параметры выборки по строкам (или бустраппинг).\n",
    "* Параметры выборки по столбцам.\n",
    "* Количество моделей.\n",
    "* Перемешивание.\n",
    "* Параметры моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Назовите очень известный алгоритм машинного обучения, построенный по принципе бэггинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Бустинг\n",
    "\n",
    "* [Бустинг](http://www.machinelearning.ru/wiki/index.php?title=%D0%91%D1%83%D1%81%D1%82%D0%B8%D0%BD%D0%B3)\n",
    "* [Boosting](https://www.coursera.org/learn/competitive-data-science/lecture/Ra7di/boosting)\n",
    "* [Открытый курс машинного обучения. Тема 10. Градиентный бустинг](https://habr.com/company/ods/blog/327250/#1-vvedenie-i-istoriya-poyavleniya-bustinga)\n",
    "\n",
    "Бустинг (англ. boosting — улучшение) — это процедура последовательного построения композиции алгоритмов машинного обучения, когда каждый следующий алгоритм стремится компенсировать недостатки композиции всех предыдущих алгоритмов. \n",
    "\n",
    "Все началось с [вопроса](http://rob.schapire.net/papers/strengthofweak.pdf) о том, можно ли из большого количества относительно слабых и простых моделей получить одну сильную. Оказалось, можно.\n",
    "\n",
    "В течение последних 10 лет бустинг остаётся одним из наиболее популярных методов машинного обучения, наряду с нейронными сетями и машинами опорных векторов. Главную роль в популяризации бустинга сыграли ML соревнования, в особенности kaggle. Основные причины — простота, универсальность, гибкость (возможность построения различных модификаций), и, главное, высокая обобщающая способность.\n",
    "\n",
    "Бустинг над решающими деревьями считается одним из наиболее эффективных методов с точки зрения качества классификации. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Основные виды бустинга:\n",
    "\n",
    "* Основанные на весах\n",
    "* Основанные на остатках"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/boosting-weights.png)\n",
    "\n",
    "Примерно так работает AdaBoost — первый популярный алгоритм бустинга. Подробнее о нем [здесь](https://habr.com/company/ods/blog/327250/#1-vvedenie-i-istoriya-poyavleniya-bustinga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/res-boosting1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/res-boosting2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/res-boosting3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Параметры бустинга \n",
    "* learning rate — как сильно мы доверяем модели? Умножаем LR на вес.\n",
    "* Количество моделей. Чем больше моделей, тем ниже LR.\n",
    "* Тип входных моделей — любая, которая принимает веса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Эта схема используется в самых популярных и эффективных библиотеках машинного обучения XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "Как работает XGBoost хорошо объясняется на сайте библиотеки, поэтому лучше поговорим об отличительных особенностях CatBoost. Названием библиотеки авторы акцентируют наше внимание на хороших результатах, которые библиотека показывает на категориальных данных. Это достигается несколькими способами:\n",
    "* [автоматическое проеобразование категориальных признаков](https://tech.yandex.com/catboost/doc/dg/features/categorical-features-docpage/).\n",
    "* mean encoding с валидацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Помимо этого в CatBoost есть ещё много приятных вещей, таких как втроенная кросс-валидация, сохранение моделей, визуализация процесса обучения, неплохая скорость работы с поддержкой GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Стекинг (Stacked Generalization или Stacking)\n",
    "\n",
    "Стекинг  был предложен [Д. Волпертом в 1992 году](http://machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf).\n",
    "\n",
    "Основная идея стекинга заключается в использовании базовых классификаторов для получения предсказаний (метапризнаков) и использовании их как признаков для некоторого ”обобщающего” алгоритма (метаалгоритма). Иными словами, основной идеей стекинга является преобразование исходного пространства признаков задачи в новое пространство, точками которого являются предсказания базовых алгоритмов. Предлагается сначала выбрать набор пар произвольных подмножеств из обучающей выборки, затем для каждой пары обучить базовые алгоритмы на первом подмножестве и предсказать ими целевую переменную для второго подмножества. Предсказанные значения и становятся объектами нового пространства. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Особенности стекинга\n",
    "\n",
    "* Поскольку в стекинге мета-модель обучается на ответах уже натренированных алгоритмов, то они сильно коррелируют. Для борьбы с этим часто базовые алгоритмы не сильно оптимизируют. Иногда здорово срабатывают идеи настройки не на целевой признак, а, например, на разницу между каким-то признаком и целевым.\n",
    "* В отличие от бустинга и традиционного бэгинга при стекинге можно (и нужно!) использовать алгоритмы разной природы (например, гребневую регрессию вместе со случайным лесом). Для формирования мета-признаков используют, как правило, регрессоры.\n",
    "* Недостаточно просто взять и состекать различные алгоритмы, нужно подстраиваться под особенности работы каждого из них. Например, если есть категориальные признаки с малым (3-4) числом категорий, то алгоритму «случайный лес» их можно подавать «как есть», а вот для регрессионных моделей нужно предварительно выполнить one-hot-кодировку.\n",
    "* Очень полезный приём, о котором часто забывают — преобразование (деформация) метапризнакового пространства. Скажем, вместо стандартных метапризнаков (ответов алгоритмов) можно использовать мономы над ними (например, все попарные произведения).\n",
    "* Стекинг можно делать многоуровневым. Готовое решение — [StackNet](https://github.com/kaz-Anova/StackNet), который является дипломной работой одного из чемпионов соревнований kaggle — kaz-Anova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple holdout scheme\n",
    "\n",
    "\n",
    "1. Разделите тренировочные данные на три части: `partA` и `partB` и `partC`.\n",
    "2. Постройте N разнородных моделей на `partA`, прогнозируйте для `partB`, `partC`, `test_data`, получите мета-признаки `partB_meta`, `partC_meta` и `test_meta` соответственно.\n",
    "3. Обучите метамодель на `partB_meta`, проверяя её гиперпараметры на `partC_meta`.\n",
    "4. Когда метамодель будет валидирована, обучите ее на [`partB_meta`, `partC_meta`] и прогнозируйте для `test_meta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Meta holdout scheme with OOF meta-features\n",
    "\n",
    "1. Разбейте тренировочную выборку `train data` на несколько частей (фолдов), затем последовательно перебирая фолды обучите N базовых алгоритмов на всех фолдах, кроме одного, а на оставшемся получите ответы базовых алгоритмов и трактуйте их как значения соответствующих признаков на этом фолде. После этого шага для каждого объекта в `train_data` у нас будет N мета-признаков (out-of-fold predictions, OOF). Назовем их `train_meta`.\n",
    "2. Обучите модели на `train data` и предскажите `test data`. Назовём эти признаки `test_meta`.\n",
    "3. Разделите `train_meta` на две части: `train_metaA` и `train_metaB`. Обучите мета-модель на `train_metaA`, провалидировав гиперпараметры на `train_metaB`.\n",
    "4. Когда мета-модель провалидирована, обучите её на `train_meta` и сделайте предсказания на `test_meta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cхема валидации честная, если набор, который мы используем для валидации метамоделей, происходит из того же распределения, что и набор мета признаков для теста. Другими словами, в честной схеме валидации набор данных, который мы используем для валидации метамоделей, никоим образом не использовался во время обучения моделей первого уровня."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Рализовывать схему стакинка вручную может быть слегка утомительно (хотя ничего мега-сложно там нет, вот [пример реализации на чисто sklearn](https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de)), поэтому можно воспользоваться библиотекой [ML-Ensemble](http://ml-ensemble.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# в ML-Ensemble стакинг сводится к этому\n",
    "from mlens.ensemble import SuperLearner \n",
    "ensemble = SuperLearner()\n",
    "ensemble.add(estimators)\n",
    "ensemble.add_meta(meta_estimator)\n",
    "ensemble.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Результаты cоревнования](https://mlbootcamp.ru/round/15/rating/)\n",
    "\n",
    "<img src='img/despair.png' width='450'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='img/joke.png' width='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='img/cross-val2.png' width='400'>\n",
    "<img src='img/cross-val.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='img/fe.png' width='400'>\n",
    "<img src='img/fe2.png' width='400'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://boosters.pro/uploads/gallery/champ21/img.png' width='450' align='right'>\n",
    "\n",
    "## Rosbank ML Competition\n",
    "\n",
    "Вам предстоит предсказать индекс популярности геолокации для размещения устройства банкоматной сети.\n",
    "\n",
    "В обучающей выборке находятся данные о геопозиции шести тысяч банкоматов Росбанка и его партнеров, а также целевая переменная — индекс популярности банкомата. В тестовой выборке еще две с половиной тысячи банкоматов, разделенных поровну на публичную и приватную часть.\n",
    "\n",
    "Мы считаем, что предложенная задача интересна и позволит поработать:\n",
    "\n",
    "* C геоданными и технологиями парсинга. Вам предстоит самому собирать геоданные, которые являются богатым источником для решения целого ряда прикладных задач. Приобретенные навыки позволят вам расширить круг решаемых задач или использовать результаты в вашей работе.\n",
    "* C небольшой выборкой. Переобучение и детектирование выбросов являются одними из серьезных проблем стоящими перед аналитиками данных. Обе они обостряются при анализе малых данных. В процессе соревнования вы получите бесценный опыт борьбы с ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Не самое обычное использование классификатора knn\n",
    "# для поиска расстояния между ближайшими банкоматами\n",
    "\n",
    "R = 6373.0 # радиус земли в километрах\n",
    "\n",
    "def distance(x,y):\n",
    "    \"\"\"\n",
    "    Haversine Formula\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    x : tuple, широта и долгота первой геокоординаты \n",
    "    y : tuple, широта и долгота второй геокоординаты \n",
    "    \n",
    "    Результат\n",
    "    ----------\n",
    "    result : дистанция в километрах между двумя геокоординатами\n",
    "    \"\"\"\n",
    "    lat_a, long_a, lat_b, long_b = map(radians, [*x,*y])    \n",
    "    dlon = long_b - long_a\n",
    "    dlat = lat_b - lat_a\n",
    "    a = sin(dlat/2)**2 + cos(lat_a) * cos(lat_b) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "knc = KNeighborsClassifier(metric=distance)\n",
    "knc.fit(X=dots , y=np.ones(dots.shape[0]))\n",
    "distances, indexes = knc.kneighbors(X=dots,n_neighbors=6,)\n",
    "\n",
    "for i in range(1,6):\n",
    "    dots['distance_%s'%i] = distances[:,i]\n",
    "    dots['indexes_%s'%i] = indexes[:,i]\n",
    "\n",
    "dots['mean'] = dots.iloc[:,dots.columns.str.contains('distance')].mean(axis=1)\n",
    "X = pd.concat([X,dots], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ссылки\n",
    "\n",
    "* [Cтекинг (Stacking) и блендинг (Blending)](https://dyakonov.org/2017/03/10/cтекинг-stacking-и-блендинг-blending/)\n",
    "* [Гущин Александр Евгеньевич. Методы ансамблирования обучающихся алгоритмов](http://www.machinelearning.ru/wiki/images/5/56/Guschin2015Stacking.pdf)\n",
    "* [Two is better than one: Ensembling Models](https://towardsdatascience.com/two-is-better-than-one-ensembling-models-611ee4fa9bd8)\n",
    "* [Boosting, Bagging, and Stacking — Ensemble Methods with sklearn and mlens](https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de)\n",
    "* [Concept of mean encoding](https://www.coursera.org/learn/competitive-data-science/lecture/b5Gxv/concept-of-mean-encoding)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
